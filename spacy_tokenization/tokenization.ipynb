{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92dc0c4-64c6-4b2f-9598-43098f0e138a",
   "metadata": {},
   "source": [
    "# Spacy tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac8571f-8701-4a2f-8ec9-e4bcdf220fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61cdecc2-a8a8-4171-b3c3-ba3d0fbdbe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Aghaei\n",
      "loves\n",
      "Tahchin\n",
      ",\n",
      "and\n",
      "it\n",
      "cost\n",
      "not\n",
      "that\n",
      "much\n",
      ",\n",
      "around\n",
      "5\n",
      "$\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')  # to know what you use here go to spacy.io/usage/models\n",
    "doc = nlp('Dr. Aghaei loves Tahchin, and it cost not that much, around 5 $') # privide the text here\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f827000b-13e9-46c6-bd7a-f5682934900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8a5adb-ecc4-4f0d-a8a8-5b0e704905e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d47048-d165-4d8c-8425-046d6e8720ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aghaei loves Tahchin, and it"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c58a201-f61d-4863-a614-9b2bff2a9040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[0]\n",
    "print(span)\n",
    "type(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2852254d-c0b3-454a-be17-902bd0103446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ea3c970-0e77-494b-b74c-c018bbf360b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.is_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63cb2e-665e-43f8-ac15-3b9cc96ad886",
   "metadata": {},
   "source": [
    "## read the file  and extract the emails from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6acdbb1a-d714-4d59-9383-cc0fcfe67ff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'file.text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m (\u001b[33m'\u001b[39m\u001b[33mfile.text\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     text = f.readlines()\n\u001b[32m      3\u001b[39m text = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, *args, **kwargs)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'file.text'"
     ]
    }
   ],
   "source": [
    "with open ('file.text') as f:\n",
    "    text = f.readlines()\n",
    "text = ''.join(text)\n",
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541ecd2-288f-4479-bf9a-9569718ea122",
   "metadata": {},
   "source": [
    "## Read a persian text and customize tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3931f2fc-4a74-4bf6-bc87-ab05f0d88e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    False\n",
      "سلام سلام False\n",
      "مُن مُن False\n",
      "این این False\n",
      "را را False\n",
      "    False\n",
      "خریدم خریدم False\n",
      "ده ده False\n",
      "تومان تومان False\n",
      "    False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank pipeline for Persian\n",
    "nlp = spacy.blank('fa')  # Use 'fa' for Farsi (Persian), NOT 'fr'\n",
    "\n",
    "# Persian text example\n",
    "text = ' سلام مُن این را  خریدم ده تومان  '\n",
    "\n",
    "# Tokenization\n",
    "doc = nlp(text)\n",
    "\n",
    "# Display token information\n",
    "for token in doc:\n",
    "    print(token, token.text, token.is_currency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16feee1a-45c9-4670-8135-4256df4e7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  False\n",
      "سلام False\n",
      "مُن False\n",
      "این False\n",
      "را False\n",
      "  False\n",
      "خریدم False\n",
      "ده False\n",
      "تومان True\n",
      "  False\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Register custom extension\n",
    "Token.set_extension(\"is_currency\", default=False, force=True)\n",
    "\n",
    "# Add rule for \"تومان\"\n",
    "for token in doc:\n",
    "    if token.text.strip() == \"تومان\":\n",
    "        token._.is_currency = True\n",
    "    print(token.text, token._.is_currency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a515645-6a94-409c-a7cb-a2611cd9522a",
   "metadata": {},
   "source": [
    "## Detect the URLs from the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08bba58f-35f0-4f7d-868f-6915fd142b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "import spacy \n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp(text)\n",
    "urls = []\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        urls.append(token.text)\n",
    "# or simply write urls = [token.text for token in doc if token.like_url]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce89d8a-312f-4774-bf5b-28b0d1cb9a40",
   "metadata": {},
   "source": [
    "## Extract all money transaction from below sentence along with currency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67691e26-53d2-43f6-aa8a-f55a23dc806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a986314-9ce7-4dbd-b54b-6691059a048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(transactions)\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text, doc[token.i+1].text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a706d-7b3e-4a88-a061-dc083fea8871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
